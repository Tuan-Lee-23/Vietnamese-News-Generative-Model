{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anp_cloned_from": {
      "notebook_id": "700375763812832",
      "revision_id": "390006458366428"
    },
    "bento_stylesheets": {
      "bento/extensions/flow/main.css": true,
      "bento/extensions/kernel_selector/main.css": true,
      "bento/extensions/kernel_ui/main.css": true,
      "bento/extensions/new_kernel/main.css": true,
      "bento/extensions/system_usage/main.css": true,
      "bento/extensions/theme/main.css": true
    },
    "disseminate_notebook_id": {
      "notebook_id": "736675493480417"
    },
    "disseminate_notebook_info": {
      "bento_version": "20191111-000215",
      "description": "BERT tutorial for OSS",
      "hide_code": false,
      "hipster_group": "",
      "kernel_build_info": {
        "deps": [
          "//aml/integrity_solutions/bento:library"
        ],
        "external_deps": []
      },
      "no_uii": true,
      "notebook_number": "173786",
      "others_can_edit": false,
      "reviewers": "",
      "revision_id": "2665524713486279",
      "tags": "",
      "tasks": "",
      "title": "Dynamic Quantization on HuggingFace BERT model"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "(experimental) Dynamic Quantization on GPT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tuan-Lee-23/Vietnamese-News-Generative-Model/blob/main/Dynamic_Quantization_on_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APjloNtd60lg"
      },
      "source": [
        "# (experimental) Dynamic Quantization on BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrnCAcuKsQME"
      },
      "source": [
        "**Author**: [Jianyu Huang](https://github.com/jianyuh)\n",
        "\n",
        "**Reviewed by**: [Raghuraman Krishnamoorthi](https://github.com/raghuramank100)\n",
        "\n",
        "**Edited by**: [Jessica Lin](https://)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWQ6E5z6IgN7"
      },
      "source": [
        "# 3. Apply the dynamic quantization\n",
        "\n",
        "We call `torch.quantization.quantize_dynamic` on the model to apply the dynamic quantization on the HuggingFace BERT model. Specifically,\n",
        "\n",
        "- We specify that we want the torch.nn.Linear modules in our model to be quantized;\n",
        "- We specify that we want weights to be converted to quantized int8 values."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tuanle/VN-News-GPT2\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"tuanle/VN-News-GPT2\")\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRMgsa3iW9gc",
        "outputId": "e86a7bc1-4a78-4704-c0ab-f0377fd0e3fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========================================================================\n",
            "Model Sizes\n",
            "===========================================================================\n",
            "FP32 Model Size: 486.77 MB\n",
            "INT8 Model Size: 523.59 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGXD0XJJYMD9",
        "outputId": "af406906-2cc9-46f8-eb8c-1c33cb5bf741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 12.8 MB 5.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from quantize_helper import QuantizeHelper\n",
        "from transformers.modeling_utils import Conv1D\n",
        "import torch\n",
        "\n",
        "def _conv1d_to_linear(module):\n",
        "    in_size, out_size = module.weight.shape\n",
        "    linear = torch.nn.Linear(in_size, out_size)\n",
        "    linear.weight.data = module.weight.data.T.contiguous()\n",
        "    linear.bias.data = module.bias.data\n",
        "    return linear\n",
        "\n",
        "\n",
        "def conv1d_to_linear(model):\n",
        "    '''in-place\n",
        "    This is for Dynamic Quantization, as Conv1D is not recognized by PyTorch, convert it to nn.Linear\n",
        "    '''\n",
        "    for name in list(model._modules):\n",
        "        module = model._modules[name]\n",
        "        if isinstance(module, Conv1D):\n",
        "            linear = _conv1d_to_linear(module)\n",
        "            model._modules[name] = linear\n",
        "        else:\n",
        "            conv1d_to_linear(module)\n",
        "\n",
        "def get_model_size(model, temp_dir=\"/tmp\"):\n",
        "\n",
        "    model_dir = os.path.join(temp_dir, \"temp\")\n",
        "    torch.save(model.state_dict(), model_dir)\n",
        "    # model.save_pretrained(model_dir)\n",
        "    size = os.path.getsize(model_dir)\n",
        "    os.remove(model_dir)\n",
        "    \n",
        "    return size\n",
        "\n",
        "\n",
        "def quantize_torch_model(model, dtype=torch.qint8):\n",
        "    '''\n",
        "    Usage: model = quantize_model(model)\n",
        "    TODO: mix of in-place and return, but results are different\n",
        "    '''\n",
        "    conv1d_to_linear(model)\n",
        "    quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=dtype)\n",
        "    return quantized_model\n",
        "\n",
        "quantized_model = quantize_torch_model(model)\n",
        "print(\"=\" * 75)\n",
        "print(\"Model Sizes\")\n",
        "print(\"=\" * 75)\n",
        "\n",
        "model_size = get_model_size(model=model)\n",
        "quantized_model_size = get_model_size(model=quantized_model)\n",
        "\n",
        "print(\"FP32 Model Size: {:.2f} MB\".format(model_size / (2 ** 20)))\n",
        "print(\"INT8 Model Size: {:.2f} MB\".format(quantized_model_size / (2 ** 20)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doerAMuzYHUT",
        "outputId": "37198fe0-0253-43da-f0ef-f20cd498c002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========================================================================\n",
            "Model Sizes\n",
            "===========================================================================\n",
            "FP32 Model Size: 486.77 MB\n",
            "INT8 Model Size: 280.62 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ0esT_qIgOC"
      },
      "source": [
        "The BERT model used in this tutorial (bert-base-uncased) has a vocabulary size V of 30522. With the embedding size of 768, the total size of the word embedding table is ~ 4 (Bytes/FP32) * 30522 * 768 = 90 MB. So with the help of quantization, the model size of the non-embedding table part is reduced from 350 MB (FP32 model) to 90 MB (INT8 model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "KNIjvg9NIgOI"
      },
      "source": [
        "Running this locally on a MacBook Pro, without quantization, inference (for all 408 examples in MRPC dataset) takes about 160 seconds, and with quantization it takes just about 90 seconds. We summarize the results for running the quantized BERT model inference on a Macbook Pro as the follows:\n",
        "\n",
        "```\n",
        "| Prec | F1 score | Model Size | 1 thread | 4 threads | \n",
        "| FP32 |  0.9019  |   438 MB   | 160 sec  | 85 sec    |\n",
        "| INT8 |  0.8953  |   181 MB   |  90 sec  | 46 sec    |\n",
        "```\n",
        "\n",
        "We have 0.6% F1 score accuracy after applying the post-training dynamic quantization on the fine-tuned BERT model on the MRPC task. As a comparison, in a [recent paper](https://arxiv.org/pdf/1910.06188.pdf) (Table 1), it achieved 0.8788 by applying the post-training dynamic quantization and 0.8956 by applying the quantization-aware training. The main difference is that we support the asymmetric quantization in PyTorch while that paper supports the symmetric quantization only.\n",
        "\n",
        "Note that we set the number of threads to 1 for the single-thread comparison in this tutorial. We also support the intra-op parallelization for these quantized INT8 operators. The users can now set multi-thread by `torch.set_num_threads(N)` (`N` is the number of intra-op parallelization threads). One preliminary requirement to enable the intra-op parallelization support is to build PyTorch with the right [backend](https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html#build-options) such as OpenMP, Native, or TBB. You can use `torch.__config__.parallel_info()` to check the parallelization settings. On the same MacBook Pro using PyTorch with Native backend for parallelization, we can get about 46 seconds for processing the evaluation of MRPC dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTQDJ8Y1syVm"
      },
      "source": [
        "## 3.3 Serialize the quantized model\n",
        "We can serialize and save the quantized model for the future use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZJH-Ln0IgOG"
      },
      "source": [
        "quantized_output_dir = configs.output_dir + \"quantized/\"\n",
        "if not os.path.exists(quantized_output_dir):\n",
        "    os.makedirs(quantized_output_dir)\n",
        "    quantized_model.save_pretrained(quantized_output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMKPDl8rIgOJ"
      },
      "source": [
        "# Conclusion\n",
        "In this tutorial, we demonstrated how to demonstrate how to convert a well-known state-of-the-art NLP model like BERT into dynamic quantized model. Dynamic quantization can reduce the size of the model while only having a limited implication on accuracy.\n",
        "\n",
        "Thanks for reading! As always, we welcome any feedback, so please create an issue [here](https://github.com/pytorch/pytorch/issues) if you have any."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfnTjseZdnvu"
      },
      "source": [
        "# References\n",
        "[1] J.Devlin, M. Chang, K. Lee and K. Toutanova, [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) (2018)\n",
        "\n",
        "[2] [HuggingFace Transformers](https://github.com/huggingface/transformers).\n",
        "\n",
        "[3] O. Zafrir, G. Boudoukh, P. Izsak, & M. Wasserblat (2019). [Q8BERT: Quantized 8bit BERT](https://arxiv.org/pdf/1910.06188.pdf).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IywQkvCIgOK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}